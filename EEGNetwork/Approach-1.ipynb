{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ADfh2D_n-_m-",
    "outputId": "7d416ddb-0167-4e56-bf12-920fad4895ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Code for Approach1: channel-to-channel reconstruction\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import  PCA\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from ReClass import Subject, TSModel\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as st\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o02oYTDszH1L"
   },
   "outputs": [],
   "source": [
    "#Create the structure of the model used for training and testing\n",
    "#input includes 50 neurons, hidden layer includes 20 neurons, \n",
    "#and output includes 1 neuron\n",
    "def createModel():  \n",
    "    classifier = keras.models.Sequential()\n",
    "    classifier.add(keras.layers.Dense(units = 20, kernel_initializer = 'normal', activation = 'relu',\\\n",
    "                         input_dim = 50))\n",
    "    classifier.add(keras.layers.Dense(units = 1, activation = 'linear'))\n",
    "\n",
    "    classifier.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "#Train the model created above\n",
    "def trainModel(x_train,y_train, scaler):\n",
    "\n",
    "    print(\"Training....\")\n",
    "    classifier = createModel()\n",
    "    x_train = scaler.fit_transform(x_train.transpose())\n",
    "    y_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "    classifier.fit(x_train, y_train, batch_size = y_train.size, epochs=20, verbose=0)\n",
    "    return classifier       \n",
    "\n",
    "#Split the subject's EEG data into training and testing sets\n",
    "#using a 75/25 rule -> 75% for trainging, 25% for testing\n",
    "def splitChannels(data):\n",
    "  train = data[:int(len(data)*0.75)]\n",
    "  test = data[int(len(data)*0.75):]\n",
    "  return train,test\n",
    "\n",
    "#Save the data in the matrix into a file\n",
    "def saveMatrix(cof,index):\n",
    "  with open(\"Sub-cof-%d.txt\" % (index), \"a+\") as f:\n",
    "    f.write(\"This is the correlation coefficient matrix for subject %d\\n\\n\" % \\\n",
    "            (index))\n",
    "    \n",
    "    for i in cof:\n",
    "      f.write(\"|\")\n",
    "      for j in i:\n",
    "        f.write(\" %f |\" % (j))\n",
    "      f.write(\"\\n\")\n",
    "\n",
    "#Only use this function if there is access to a GPU   \n",
    "def reset_keras():\n",
    "  sess = get_session()\n",
    "  clear_session()\n",
    "  sess.close()\n",
    "  sess = get_session()\n",
    "  \n",
    "  try:\n",
    "    del classifier\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  print(gc.collect())\n",
    "  \n",
    "  config = tf.ConfigProto()\n",
    "  config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "  config.gpu_options.visible_device_list = \"0\"\n",
    "  set_session(tf.Session(config=config))\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  \n",
    "  subjects = [] #Creating a list of Subject classes to keep track of participant information\n",
    "  \n",
    "  #separate the .mat files needed from the directories\n",
    "  subjectData = sorted(glob.glob('/content/drive/My Drive/Spark-2019/AD_MCI_Data/reformated data/resting data/extracted data 30chns/rest1 1min/rec*.mat'))#+ glob.glob('rec??.mat'))\n",
    "  subjectData = [x[108:] for x in subjectData]\n",
    "  \n",
    "  #Loop through each file and save the subjects data, state, and channels in a Class Object\n",
    "  for index,value in enumerate(subjectData):\n",
    "      \n",
    "      subjects.append(Subject(loadmat('/content/drive/My Drive/Spark-2019/AD_MCI_Data/reformated data/resting data/headerInfo'+value)['dx'][0],\\\n",
    "                              loadmat('/content/drive/My Drive/Spark-2019/AD_MCI_Data/reformated data/resting data/extracted data 30chns/rest1 1min'+value)['data'],\\\n",
    "                              index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5QxgxZMJLyde",
    "outputId": "ef00167c-c5b3-4250-aef2-c4125d7ee342"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 15:21:24.419510 139907676432256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0710 15:21:24.422183 139907676432256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0710 15:21:24.424314 139907676432256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0710 15:21:24.450756 139907676432256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0710 15:21:24.452704 139907676432256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0710 15:21:24.697018 139907676432256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0710 15:21:24.735639 139907676432256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "0\n",
      "31\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "1\n",
      "640\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "2\n",
      "1565\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "3\n",
      "640\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n",
      "Training....\n"
     ]
    }
   ],
   "source": [
    "  models = [[None]*30]*30\n",
    "  cof = np.ones((30,30))\n",
    "  scaler = StandardScaler()\n",
    "  \n",
    "  #Loop through each subject and create a 30x30 causality matrix\n",
    "  #Channel-to-channel reconstruction is used for each model\n",
    "  #A right-shifting method is used on the EEG data to create enough input variables for the model\n",
    "  #Reconstruct the EEG data, calculate the correlation coefficient\n",
    "  #between the original and reconstructed data, and save the data in a file\n",
    "  for s in subjects:\n",
    "    \n",
    "    for i,v1 in enumerate(s.data.transpose()):\n",
    "      #reset_keras()\n",
    "      train1,test1 = splitChannels(v1)\n",
    "      tmp = train1\n",
    "\n",
    "      for k in range(49):\n",
    "        train1 = np.pad(train1,(1,0), mode='constant')[:-1]\n",
    "        tmp = np.vstack((tmp,train1))\n",
    "      train1 = tmp\n",
    "\n",
    "      tmp = test1\n",
    "      for k in range(49):\n",
    "        test1 = np.pad(test1,(1,0), mode='constant')[:-1]\n",
    "        tmp = np.vstack((tmp,test1))\n",
    "      test1 = tmp\n",
    "\n",
    "      for j,v2 in enumerate(s.data.transpose()):\n",
    "          \n",
    "          train2,test2 = splitChannels(v2)\n",
    "          models[i][j] = trainModel(train1,train2, scaler)\n",
    "\n",
    "          pred = models[i][j].predict(x=test1.transpose())\n",
    "        \n",
    "          if i != j:\n",
    "            cof[i][j] = st.pearsonr(pred.flatten('C'),test2.transpose())[0]\n",
    "      print(i)\n",
    "    saveMatrix(cof,s.index)\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "right_shift_method.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
